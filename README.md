# awesome-embodied-vla/va/vln [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

As more and more outstanding vision-language-based policies emerge, this repository aims to organize and showcase the state-of-the-art technologies in robot learning, including vision-language-action (VLA) models, vision-language-navigation (VLN) models, vision-action (VA) models and other MLLM-based embodied learning. We hope that in the near future, robotics will experience its own 'LLM moment.'

This repository will be continuously updated, and we warmly invite contributions from the community. If you have any papers, projects, or resources that are not yet included, please feel free to submit them via a pull request or open an issue for discussion. 

Let’s build a comprehensive resource for the robotics and AI community!

*Jony and Sage*


## Survey
- [2025] Development Report of Embodied Intelligence (Chinese) [[paper](https://www.caict.ac.cn/kxyj/qwfb/bps/202408/P020240830312499650772.pdf)]
- [2025] Survey on Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2502.06851)]
- [2025] Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions [[paper](https://arxiv.org/pdf/2502.15336)]
- [2024] Embodied-AI with large models: research and challenges [[paper](https://www.sciengine.com/SSI/doi/10.1360/SSI-2024-0076)]
- [2024] A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- [2024] A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- [2024] Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- [2024] Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]

## Vision Language Action (VLA) Models

### 2025 


- [2025] Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success [[paper](https://arxiv.org/pdf/2502.19645)] [[project](https://openvla-oft.github.io/)] 
- [2025] Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2502.19417)] [[project](https://www.pi.website/research/hirobot)] 
- [2025] Helix: A Vision-Language-Action Model for Generalist Humanoid Control [[report](https://www.figure.ai/news/helix)]
- [2025] EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation [[paper](https://arxiv.org/pdf/2501.01895)]
- [2025] Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing [[paper](https://arxiv.org/pdf/2501.06919)]
- [2025] Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding [[paper](https://arxiv.org/pdf/2501.04693)]
- [2025] FAST: Efficient Action Tokenization for Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2501.09747)]
- [2025] GeoManip: Geometric Constraints as General Interfaces for Robot Manipulation [[paper](https://arxiv.org/pdf/2501.09783)]
- [2025] Universal Actions for Enhanced Embodied Foundation Models [[paper](https://arxiv.org/pdf/2501.10105)]
- [2025] SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model [[paper](https://arxiv.org/pdf/2501.15830)]
- [2025] RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon Robotic Manipulation [[paper](https://arxiv.org/pdf/2501.06605)]
- [2025] SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation [[paper](https://arxiv.org/pdf/2501.18564)]
- [2025] Improving Vision-Language-Action Model with Online Reinforcement Learning [[paper](https://arxiv.org/pdf/2501.16664)]
- [2025] Integrating LMM Planners and 3D Skill Policies for Generalizable Manipulation [[paper](https://arxiv.org/pdf/2501.18733)]
- [2025] VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation [[paper](https://arxiv.org/pdf/2502.02175)]
- [2025] From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment [[paper](https://arxiv.org/pdf/2502.01828)]
- [2025] GRAPE: Generalizing Robot Policy via Preference Alignment [[paper](https://arxiv.org/pdf/2411.19309)]
- [2025] DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control [[paper](https://arxiv.org/pdf/2502.05855)]
- [2025] HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation [[paper](https://arxiv.org/pdf/2502.05485)]
- [2025] Temporal Representation Alignment: Successor Features Enable Emergent Compositionality in Robot Instruction Following Temporal Representation Alignment [[paper](https://arxiv.org/pdf/2502.05454)]
- [2025] ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy [[paper](https://arxiv.org/pdf/2502.05450)]
- [2025] RoboBERT: An End-to-end Multimodal Robotic Manipulation Model [[paper](https://arxiv.org/pdf/2502.07837)]
- [2025] Diffusion Transformer Policy: Scaling Diffusion Transformer for Generalist Visual-Language-Action Learning [[paper](https://arxiv.org/pdf/2410.15959)]
- [2025] GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation [[paper](https://arxiv.org/pdf/2502.09268)]
- [2025] SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation [[paper](https://arxiv.org/pdf/2502.09268)]
- [2025] Pre-training Auto-regressive Robotic Models with 4D Representations [[paper](https://arxiv.org/pdf/2502.13142)]
- [2025] Magma: A Foundation Model for Multimodal AI Agents [[paper](https://arxiv.org/pdf/2502.13130)]
- [2025] An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation [[paper](https://arxiv.org/pdf/2501.15068)]
- [2025] VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation [[paper](https://arxiv.org/pdf/2502.13508v1)]
- [2025] Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration [[paper](https://arxiv.org/pdf/2502.14795)]
- [2025] ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2502.14420)]
- [2025] ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration [[paper](https://arxiv.org/pdf/2502.19250)] [[project](https://objectvla.github.io/)] 


### 2024

- [2024] π0: A Vision-Language-Action Flow Model for General Robot Control [[paper](https://www.physicalintelligence.company/download/pi0.pdf)]
- [2024] RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation [[paper](https://arxiv.org/pdf/2410.07864)]
- [2024] OpenVLA: An Open-Source Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2406.09246)]
- [2024] Octo: An Open-Source Generalist Robot Policy [[paper](https://arxiv.org/pdf/2405.12213)]
- [2024] Open X-Embodiment: Robotic Learning Datasets and RT-X Models [[paper](https://arxiv.org/pdf/2310.08864)]
- [2024] RT-H: Action Hierarchies Using Language [[paper](https://arxiv.org/pdf/2403.01823v1)]
- [2024] Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models [[paper](https://arxiv.org/abs/2412.14058)]
- [2024] Open X-Embodiment: Robotic Learning Datasets and RT-X Models [[paper](https://arxiv.org/pdf/2310.08864)]
- [2024] Baku: An Efficient Transformer for Multi-Task Policy Learning [[paper](https://arxiv.org/pdf/2406.07539v1)]
- [2024] Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals [[paper](https://arxiv.org/pdf/2407.05996)]
- [2024] TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation [[paper](https://arxiv.org/pdf/2409.12514)]
- [2024] Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression [[paper](https://arxiv.org/pdf/2412.03293)]
- [2024] CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation [[paper](https://www.arxiv.org/pdf/2411.19650)]
- [2024] 3D-VLA: A 3D Vision-Language-Action Generative World Model [[paper](https://arxiv.org/pdf/2403.09631)]
- [2024] Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations [[paper](https://arxiv.org/pdf/2405.06039)]
- [2024] An Embodied Generalist Agent in 3D World [[paper](https://arxiv.org/pdf/2311.12871)]
- [2024] RoboMM: All-in-One Multimodal Large Model for Robotic Manipulation [[paper](https://arxiv.org/pdf/2412.07215)]
- [2024] SpatialBot: Precise Spatial Understanding with Vision Language Models [[paper](https://arxiv.org/pdf/2406.13642)]
- [2024] Depth Helps: Improving Pre-trained RGB-based Policy with Depth Information Injection [[paper](https://arxiv.org/pdf/2408.05107v1)]
- [2024] HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers [[paper](https://arxiv.org/pdf/2410.05273)]
- [2024] LLaRA: Supercharging Robot Learning Data for Vision-Language Policy [[paper](https://arxiv.org/pdf/2406.20095)]
- [2024] RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulation [[paper](https://arxiv.org/pdf/2406.18977)]
- [2024] Robotic Control via Embodied Chain-of-Thought Reasoning [[paper](https://arxiv.org/pdf/2407.08693)]
- [2024] GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation [[paper](https://arxiv.org/pdf/2410.06158)]
- [2024] Latent Action Pretraining from Videos [[paper](https://arxiv.org/pdf/2410.11758)]
- [2024] DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution [[paper](https://arxiv.org/pdf/2411.02359)]
- [2024] RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation [[paper](https://arxiv.org/pdf/2411.02704)]
- [2024] Moto: Latent Motion Token as the Bridging Language for Robot Manipulation [[paper](https://arxiv.org/pdf/2412.04445)]
- [2024] TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies [[paper](https://arxiv.org/pdf/2412.10345)]
- [2024] Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments [[paper](https://arxiv.org/pdf/2409.05865)]
- [2024] Moto: Latent Motion Token as the Bridging Language for Robot Manipulation [[paper](https://arxiv.org/pdf/2412.04445)]
- [2024] RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics [[paper](https://arxiv.org/pdf/2406.10721)]
- [2024] Yell At Your Robot: Improving On-the-Fly from Language Corrections [[paper](https://arxiv.org/pdf/2403.12910)]
- [2024] Any-point Trajectory Modeling for Policy Learning [[paper](https://arxiv.org/pdf/2401.00025)]
- [2024] Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust [[paper](https://arxiv.org/pdf/2410.01971)]
- [2024] RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2409.19590)]
- [2024] Actra: Optimized Transformer Architecture for Vision-Language-Action Models in Robot Learning [[paper](https://arxiv.org/pdf/2408.01147)]
- [2024] QUAR-VLA: Vision-Language-Action Model for Quadruped Robots [[paper](https://arxiv.org/pdf/2312.14457)]
- [2024] RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2409.19590)]
- [2024] DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution [[paper](https://arxiv.org/pdf/2411.02359)]



### 2023 

- [2023] RT-1: Robotics Transformer for Real-World Control at Scale [[paper](https://arxiv.org/pdf/2212.06817)]
- [2023] RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control [[paper](https://arxiv.org/pdf/2307.15818)]
- [2023] PaLM-E: An Embodied Multimodal Language Model: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control [[paper](https://arxiv.org/pdf/2303.03378)]
- [2023] Vision-Language Foundation Models as Effective Robot Imitators [[paper](https://arxiv.org/pdf/2311.01378)]
- [2023] Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation [[paper](https://arxiv.org/pdf/2312.13139)]
- [2023] Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models [[paper](https://arxiv.org/pdf/2310.10639)]
- [2023] Learning Universal Policies via Text-Guided Video Generation [[paper](https://arxiv.org/pdf/2302.00111)]
- [2023] Learning to Act from Actionless Videos through Dense Correspondences [[paper](https://arxiv.org/pdf/2310.08576)]
- [2023] Compositional Foundation Models for Hierarchical Planning [[paper](https://arxiv.org/pdf/2309.08587)]
- [2023] VIMA: General Robot Manipulation with Multimodal Prompts [[paper](https://vimalabs.github.io./assets/vima_paper.pdf)]
- [2023] Prompt a Robot to Walk with Large Language Models [[paper](https://arxiv.org/pdf/2309.09969)]
- [2023] Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning [[paper](https://arxiv.org/pdf/2311.17842)]


## Vision Language Navigation (VLN) Models

### 2025

- [2025] Semantic Mapping in Indoor Embodied AI - A Comprehensive Survey and Future Directions [[paper](https://arxiv.org/pdf/2501.05750)]
- [2025] VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning [[paper](https://arxiv.org/pdf/2502.00931)]
- [2025] TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation [[paper](https://arxiv.org/pdf/2502.07306)]
- [2025] VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion [[paper](https://arxiv.org/pdf/2502.01536)]
- [2025] NavigateDiff: Visual Predictors are Zero-Shot Navigation Assistants [[paper](https://arxiv.org/pdf/2502.13894)]
- [2025] MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation [[paper](https://arxiv.org/pdf/2502.13451)]
- [2025] OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation [[paper](https://arxiv.org/pdf/2502.18041)]
- [2025] Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments [[paper](https://arxiv.org/pdf/2502.19024)]
- [2025] UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation [[paper](https://arxiv.org/pdf/2501.05014)]


### 2024

- [2024] Navid: Video-based vlm plans the next step for vision-andlanguage navigation [[paper](https://arxiv.org/pdf/2402.15852)]
- [2024] NaVILA: Legged Robot Vision-Language-Action Model for Navigation [[paper](https://arxiv.org/pdf/2412.04453)]
- [2024] The One RING: a Robotic Indoor Navigation Generalist [[paper](https://arxiv.org/pdf/2412.14401)]
- [2024] Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs [[paper](https://arxiv.org/pdf/2407.07775)]

### 2023

- [2023] Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill [[paper](https://arxiv.org/pdf/2309.10309)]


## Vision Action (VA) Models

### 2025

- [2025] Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics [[paper](https://arxiv.org/pdf/2501.10100)]
- [2025] You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations [[paper](https://arxiv.org/pdf/2501.14208)]
- [2025] ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills [[paper](https://arxiv.org/pdf/2502.01143)]
- [2025] VILP: Imitation Learning with Latent Video Planning [[paper](https://arxiv.org/pdf/2502.01784)]
- [2025] Learning the RoPEs: Better 2D and 3D Position Encodings with STRING [[paper](https://arxiv.org/pdf/2502.02562)]
- [2025] When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning [[paper](https://arxiv.org/pdf/2502.03270)]
- [2025] RoboGrasp: A Universal Grasping Policy for Robust Robotic Control [[paper](https://arxiv.org/pdf/2502.03072)]
- [2025] CordViP: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World [[paper](https://arxiv.org/pdf/2502.08449)]
- [2025] Learning to Group and Grasp Multiple Objects [[paper](https://arxiv.org/pdf/2502.08452)]
- [2025] Beyond Behavior Cloning: Robustness through Interactive Imitation and Contrastive Learning [[paper](https://arxiv.org/pdf/2502.07645)]
- [2025] COMBO-Grasp: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping [[paper](https://arxiv.org/pdf/2502.08054)]
- [2025] DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References [[paper](https://arxiv.org/pdf/2502.09614)]
- [2025] S2-Diffusion: Generalizing from Instance-level to Category-level Skills in Robot Manipulation [[paper](https://arxiv.org/pdf/2502.09389)]
- [2025] MTDP: Modulated Transformer Diffusion Policy Model [[paper](https://arxiv.org/pdf/2502.09029)]
- [2025] FUNCTO: Function-Centric One-Shot Imitation Learning for Tool Manipulation [[paper](https://arxiv.org/pdf/2502.11744)]
- [2025] RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations [[paper](https://arxiv.org/pdf/2502.13134)]
- [2025] Responsive Noise-Relaying Diffusion Policy: Responsive and Efficient Visuomotor Control [[paper](https://arxiv.org/pdf/2502.12724)]
- [2025] Learning a High-quality Robotic Wiping Policy Using Systematic Reward Analysis and Visual-Language Model Based Curriculum [[paper](https://arxiv.org/pdf/2502.12599)]
- [2025] IMLE Policy: Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation [[paper](https://arxiv.org/pdf/2502.12371)]
- [2025] X-IL: Exploring the Design Space of Imitation Learning Policies [[paper](https://arxiv.org/pdf/2502.12330)]
- [2025] Towards Fusing Point Cloud and Visual Representations for Imitation Learning [[paper](https://arxiv.org/pdf/2502.12320)]
- [2025] Pick-and-place Manipulation Across Grippers Without Retraining: A Learning-optimization Diffusion Policy Approach [[paper](https://arxiv.org/pdf/2502.15613)]
- [2025] FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning [[paper](https://arxiv.org/pdf/2502.17432)]
- [2025] DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning [[paper](https://arxiv.org/pdf/2502.16932)]
- [2025] Human2Robot: Learning Robot Actions from Paired Human-Robot Videos [[paper](https://arxiv.org/pdf/2502.16587)]
- [2025] AnyDexGrasp: General Dexterous Grasping for Different Hands with Human-level Learning Efficiency [[paper](https://arxiv.org/pdf/2502.16420)]
- [2025] COMPASS: Cross-embOdiment Mobility Policy via ResiduAl RL and Skill Synthesis [[paper](https://arxiv.org/pdf/2502.16372)]
- [2025] Retrieval Dexterity: Efficient Object Retrieval in Clutters with Dexterous Hand [[paper](https://arxiv.org/pdf/2502.18423)]
- [2025] From planning to policy: distilling Skill-RRT for long-horizon prehensile and non-prehensile manipulation [[paper](https://arxiv.org/pdf/2502.18015)]
- [2025] FetchBot: Object Fetching in Cluttered Shelves via Zero-Shot Sim2Real [[paper](https://arxiv.org/pdf/2502.17894)]
- [2025] Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation [[paper](https://arxiv.org/pdf/2502.20391)] [[project](https://www.pi.website/research/hirobot)] 
- [2025] FuseGrasp: Radar-Camera Fusion for Robotic Grasping of Transparent Objects [[paper](https://arxiv.org/pdf/2502.20037)]
- [2025] Sensor-Invariant Tactile Representation [[paper](https://arxiv.org/pdf/2502.19638)]
- [2025] Generalist World Model Pre-Training for Efficient Reinforcement Learning [[paper](https://arxiv.org/pdf/2502.19544)]



### 2024

- [2024] Learning Robotic Manipulation Policies from Point Clouds with Conditional Flow Matching [[paper](https://arxiv.org/pdf/2409.07343)]
- [2024] Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning [[paper](https://arxiv.org/abs/2402.02500)]
- [2024] 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations [[paper](https://arxiv.org/pdf/2403.03954)]
- [2024] Sparse diffusion policy: A sparse, reusable, and flexible policy for robot learning [[paper](https://arxiv.org/pdf/2407.01531)]
- [2024] ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation [[paper](https://arxiv.org/pdf/2406.01586)]
- [2024] 3d diffuser actor: Policy diffusion with 3d scene representations [[paper](https://arxiv.org/pdf/2402.10885)]
- [2024] Diffusion Policy Policy Optimization [[paper](https://arxiv.org/pdf/2409.00588)]
- [2024] Language-Guided Object-Centric Diffusion Policy for Collision-Aware Robotic Manipulation [[paper](https://arxiv.org/pdf/2407.00451)]
- [2024] EquiBot: SIM(3)-Equivariant Diffusion Policy for Generalizable and Data Efficient Learning [[paper](https://arxiv.org/pdf/2407.01479)]
- [2024] Equivariant Diffusion Policy [[paper](https://arxiv.org/pdf/2407.01812)]
- [2024] Mamba Policy: Towards Efficient 3D Diffusion Policy with Hybrid Selective State Models [[paper](https://arxiv.org/pdf/2409.07163)]
- [2024] Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies [[paper](https://arxiv.org/pdf/2410.10803)]
- [2024] Motion Before Action: Diffusing Object Motion as Manipulation Condition [[paper](https://arxiv.org/pdf/2411.09658)]
- [2024] One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation [[paper](https://arxiv.org/pdf/2410.21257)]
- [2024] Consistency policy: Accelerated visuomotor policies via consistency distillation [[paper](https://arxiv.org/pdf/2405.07503)]
- [2024] SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation [[paper](https://arxiv.org/pdf/2411.00965)]
- [2024] RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins [[paper](https://arxiv.org/pdf/2409.02920)]
- [2024] Few-Shot Task Learning through Inverse Generative Modeling [[paper](https://arxiv.org/pdf/2411.04987?)]
- [2024] G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation [[paper](https://arxiv.org/pdf/2411.18369)]
- [2024] Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation [[paper](https://arxiv.org/pdf/2410.08001)]
- [2024] Diffusion Policy Attacker: Crafting Adversarial Attacks for Diffusion-based Policies [[paper](https://arxiv.org/pdf/2405.19424)]
- [2024] Imagination Policy: Using Generative Point Cloud Models for Learning Manipulation Policies [[paper](https://arxiv.org/pdf/2406.11740)]
- [2024] Equivariant diffusion policy [[paper](https://arxiv.org/pdf/2407.01812)]
- [2024] Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation [[paper](https://arxiv.org/pdf/2409.14411)]
- [2024] Data Scaling Laws in Imitation Learning for Robotic Manipulation [[paper](https://arxiv.org/pdf/2410.18647)]
- [2024] Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation [[paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Hierarchical_Diffusion_Policy_for_Kinematics-Aware_Multi-Task_Robotic_Manipulation_CVPR_2024_paper.pdf)]
- [2024] Equivariant diffusion policy [[paper](https://arxiv.org/pdf/2407.01812)]
- [2024] Learning universal policies via text-guided video generation [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/1d5b9233ad716a43be5c0d3023cb82d0-Paper-Conference.pdf)]
- [2024] Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning [[paper](https://arxiv.org/pdf/2307.01849)]
- [2024] 3D Diffuser Actor: Policy Diffusion with 3D Scene Representations [[paper](https://arxiv.org/pdf/2402.10885)]
- [2024] Act3D: 3D Feature Field Transformers for Multi-Task Robotic Manipulation [[paper](https://arxiv.org/pdf/2306.17817)]
- [2024] GenDP: 3D Semantic Fields for Category-Level Generalizable Diffusion Policy [[paper](https://arxiv.org/pdf/2410.17488)]
- [2024] Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for Robust 3D Robotic Manipulation [[paper](https://arxiv.org/pdf/2411.18623)]
- [2024] Prediction with Action: Visual Policy Learning via Joint Denoising Process [[paper](https://arxiv.org/pdf/2411.18179)]
- [2024] Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations [[paper](https://arxiv.org/pdf/2412.14803)]
- [2024] Bidirectional Decoding: Improving Action Chunking via Closed-Loop Resampling [[paper](https://arxiv.org/pdf/2408.17355)]
- [2024] Streaming Diffusion Policy: Fast Policy Synthesis with Variable Noise Diffusion Models [[paper](https://arxiv.org/pdf/2406.04806)]
- [2024] CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction [[paper](https://arxiv.org/pdf/2412.06782)]


### 2023

- [2023] Diffusion policy: Visuomotor policy learning via action diffusion [[paper](https://arxiv.org/pdf/2303.04137)]
- [2023] Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods [[paper](https://arxiv.org/pdf/2308.03620)]

## Other Multimodal Large Language Model (MLLM)-based/related Embodied Learning

### 2025
- [2025] Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation [[paper](https://arxiv.org/pdf/2502.16707v1)]
- [2025] RoboGrasp: A Universal Grasping Policy for Robust Robotic Control [[paper](https://arxiv.org/pdf/2502.03072)]
- [2025] VLP: Vision-Language Preference Learning for Embodied Manipulation [[paper](https://arxiv.org/pdf/2502.11918)]
- [2025] 3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds [[paper](https://arxiv.org/pdf/2502.20041)]


### 2024
- [2024] AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation [[paper](https://arxiv.org/pdf/2406.11548)]
- [2024] OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints [[paper](https://arxiv.org/pdf/2501.03841)]
- [2024] Grasp What You Want: Embodied Dexterous Grasping System Driven by Your Voice [[paper](https://arxiv.org/pdf/2412.10694)]
- [2024] Towards Open-World Grasping with Large Vision-Language Models [[paper](https://arxiv.org/pdf/2406.18722v4)]
- [2024] ThinkGrasp: A Vision-Language System for Strategic Part Grasping in Clutter [[paper](https://arxiv.org/pdf/2407.11298v1)]
- [2024] MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World [[paper](https://arxiv.org/pdf/2401.08577)]
- [2024] Physically Grounded Vision-Language Models for Robotic Manipulation [[paper](https://arxiv.org/pdf/2309.02561)]
- [2024] Eureka: Human-Level Reward Design via Coding Large Language Models [[paper](https://arxiv.org/pdf/2310.12931)]
- [2024] Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration [[paper](https://arxiv.org/pdf/2405.14314)]
- [2024] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [[paper](https://say-can.github.io/assets/palm_saycan.pdf)]


### 2023
- [2023] LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models [[paper](https://arxiv.org/pdf/2212.04088)]
- [2023] LLM+P: Empowering Large Language Models with Optimal Planning Proficiency [[paper](https://arxiv.org/pdf/2304.11477)]
- [2023] Code as Policies: Language Model Programs for Embodied Control [[paper](https://arxiv.org/pdf/2209.07753)]

### 2022
- [2022] Inner Monologue: Embodied Reasoning through Planning with Language Models [[paper](https://arxiv.org/pdf/2207.05608)]


## Physics-aware Policy
- [2025] Surface-Based Manipulation [[paper](https://arxiv.org/pdf/2502.19389)]

## Sim-to-Real Transfer

- [2025] RE3SIM: Generating High-Fidelity Simulation Data via 3D-Photorealistic Real-to-Sim for Robotic Manipulation [[paper](https://arxiv.org/pdf/2502.08645)]
- [2025] VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion [[paper](https://arxiv.org/pdf/2502.01536)]
- [2025] A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards [[paper](https://arxiv.org/pdf/2502.08643)]
- [2025] A Distributional Treatment of Real2Sim2Real for Vision-Driven Deformable Linear Object Manipulation [[paper](https://arxiv.org/pdf/2502.18615)]
- [2025] Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids [[paper](https://arxiv.org/pdf/2502.20396)] [[paper](https://toruowo.github.io/recipe/)]

## Benchmark
- [2025] OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation [[paper](https://arxiv.org/pdf/2502.18041)]
- [2025] BOSS: Benchmark for Observation Space Shift in Long-Horizon Task [[paper](http://arxiv.org/pdf/2502.15679)]
- [2025] OpenBench: A New Benchmark and Baseline for Semantic Navigation in Smart Logistics [[paper](https://arxiv.org/pdf/2502.09238)]
- [2024] EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models [[paper](https://arxiv.org/abs/2406.05756)]
- [2024] VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Task [[paper](https://arxiv.org/pdf/2412.18194)]
- [2024] Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations [[paper](https://arxiv.org/pdf/2402.14606)]

## Simulator
- [2025] MuJoCo Playground [[paper](https://arxiv.org/pdf/2502.08844)]
- [2024] Genesis: A Generative and Universal Physics Engine for Robotics and Beyond [[paper](https://genesis-embodied-ai.github.io/)]
- [2024] Maniskill V1-V3 [[v1](https://arxiv.org/pdf/2107.14483)] [[v2](https://arxiv.org/pdf/2302.04659)] [[v3](https://arxiv.org/pdf/2410.00425)]
- [2024] Nvidia Isaac [[Isaac Lab](https://github.com/isaac-sim/IsaacLab)] [[Isaac Sim](https://docs.isaacsim.omniverse.nvidia.com/latest/index.html)] [[Isaac Gym](https://developer.nvidia.com/isaac-gym)]
- [2022] Mojoco [[document](https://mujoco.readthedocs.io/en/stable/overview.html)]


## Ralated Works
- Awesome-Generalist-Agents [[repo](https://github.com/cheryyunl/awesome-generalist-agents)]
- Awesome-LLM-Robotics [[repo](https://github.com/GT-RIPL/Awesome-LLM-Robotics)]
- Awesome World Models for Robotics [[repo](https://github.com/leofan90/Awesome-World-Models)]


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=jonyzhang2023/awesome-embodied-vla-va-vln&type=Date)](https://star-history.com/#jonyzhang2023/awesome-embodied-vla-va-vln&Date)