# awesome-embodied-vla/va/vln [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)


### Survey
- [2024] A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- [2024] A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- [2024] Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]


## Vision Language Action (VLA) Models

### 2025 

- [2025] EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation [[paper](https://arxiv.org/pdf/2501.01895)]
- [2025] Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing [[paper](https://arxiv.org/pdf/2501.06919)]
- [2025] Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding [[paper](https://arxiv.org/pdf/2501.04693)]
- [2025] FAST: Efficient Action Tokenization for Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2501.09747)]
- [2025] GeoManip: Geometric Constraints as General Interfaces for Robot Manipulation [[paper](https://arxiv.org/pdf/2501.09783)]
- [2025] Universal Actions for Enhanced Embodied Foundation Models [[paper](https://arxiv.org/pdf/2501.10105)]

### 2024

- [2024] π0: A Vision-Language-Action Flow Model for General Robot Control [[paper](https://www.physicalintelligence.company/download/pi0.pdf)]
- [2024] RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation [[paper](https://arxiv.org/pdf/2410.07864)]
- [2024] OpenVLA: An Open-Source Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2406.09246)]
- [2024] Octo: An Open-Source Generalist Robot Policy [[paper](https://arxiv.org/pdf/2405.12213)]
- [2024] Open X-Embodiment: Robotic Learning Datasets and RT-X Models [[paper](https://arxiv.org/pdf/2310.08864)]
- [2024] RT-H: Action Hierarchies Using Language [[paper](https://arxiv.org/pdf/2403.01823v1)]
- [2024] Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models [[paper](https://arxiv.org/abs/2412.14058)]
- [2024] Open X-Embodiment: Robotic Learning Datasets and RT-X Models [[paper](https://arxiv.org/pdf/2310.08864)]
- [2024] Baku: An Efficient Transformer for Multi-Task Policy Learning [[paper](https://arxiv.org/pdf/2406.07539v1)]
- [2024] Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals [[paper](https://arxiv.org/pdf/2407.05996)]
- [2024] TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation [[paper](https://arxiv.org/pdf/2409.12514)]
- [2024] Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression [[paper](https://arxiv.org/pdf/2412.03293)]
- [2024] CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation [[paper](https://www.arxiv.org/pdf/2411.19650)]
- [2024] 3D-VLA: A 3D Vision-Language-Action Generative World Model [[paper](https://arxiv.org/pdf/2403.09631)]
- [2024] Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations [[paper](https://arxiv.org/pdf/2405.06039)]
- [2024] An Embodied Generalist Agent in 3D World [[paper](https://arxiv.org/pdf/2311.12871)]
- [2024] RoboMM: All-in-One Multimodal Large Model for Robotic Manipulation [[paper](https://arxiv.org/pdf/2412.07215)]
- [2024] SpatialBot: Precise Spatial Understanding with Vision Language Models [[paper](https://arxiv.org/pdf/2406.13642)]
- [2024] Depth Helps: Improving Pre-trained RGB-based Policy with Depth Information Injection [[paper](https://arxiv.org/pdf/2408.05107v1)]
- [2024] HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers [[paper](https://arxiv.org/pdf/2410.05273)]




### 2023 
- [2023] RT-1: Robotics Transformer for Real-World Control at Scale [[paper](https://arxiv.org/pdf/2212.06817)]
- [2023] RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control [[paper](https://arxiv.org/pdf/2307.15818)]


## Vision Language Navigation (VLN) Models

### 2025

- [2025] Semantic Mapping in Indoor Embodied AI – A Comprehensive Survey and Future Directions [[paper](https://arxiv.org/pdf/2501.05750)]


### 2024

- [2024] Navid: Video-based vlm plans the next step for vision-andlanguage navigation [[paper](https://arxiv.org/pdf/2402.15852)]
- [2024] NaVILA: Legged Robot Vision-Language-Action Model for Navigation [[paper](https://arxiv.org/pdf/2412.04453)]
- [2024] The One RING: a Robotic Indoor Navigation Generalist [[paper](https://arxiv.org/pdf/2412.14401)]

## Vision Action (VA) Models

### 2025

- [2025] Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics [[paper](https://arxiv.org/pdf/2501.10100)]


### 2024

- [2024] 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations [[paper](https://arxiv.org/pdf/2403.03954)]
- [2024] Sparse diffusion policy: A sparse, reusable, and flexible policy for robot learning [[paper](https://arxiv.org/pdf/2407.01531)]
- [2024] ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation [[paper](https://arxiv.org/pdf/2406.01586)]
- [2024] 3d diffuser actor: Policy diffusion with 3d scene representations [[paper](https://arxiv.org/pdf/2402.10885)]
- [2024] Diffusion Policy Policy Optimization [[paper](https://arxiv.org/pdf/2409.00588)]
- [2024] Language-Guided Object-Centric Diffusion Policy for Collision-Aware Robotic Manipulation [[paper](https://arxiv.org/pdf/2407.00451)]
- [2024] EquiBot: SIM(3)-Equivariant Diffusion Policy for Generalizable and Data Efficient Learning [[paper](https://arxiv.org/pdf/2407.01479)]
- [2024] Equivariant Diffusion Policy [[paper](https://arxiv.org/pdf/2407.01812)]
- [2024] Mamba Policy: Towards Efficient 3D Diffusion Policy with Hybrid Selective State Models [[paper](https://arxiv.org/pdf/2409.07163)]
- [2024] Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies [[paper](https://arxiv.org/pdf/2410.10803)]
- [2024] Motion Before Action: Diffusing Object Motion as Manipulation Condition [[paper](https://arxiv.org/pdf/2411.09658)]
- [2024] One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation [[paper](https://arxiv.org/pdf/2410.21257)]
- [2024] Consistency policy: Accelerated visuomotor policies via consistency distillation [[paper](https://arxiv.org/pdf/2405.07503)]
- [2024] SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation [[paper](https://arxiv.org/pdf/2411.00965)]
- [2024] RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins [[paper](https://arxiv.org/pdf/2409.02920)]
- [2024] Few-Shot Task Learning through Inverse Generative Modeling [[paper](https://arxiv.org/pdf/2411.04987?)]
- [2024] G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation [[paper](https://arxiv.org/pdf/2411.18369)]
- [2024] Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation [[paper](https://arxiv.org/pdf/2410.08001)]
- [2024] Diffusion Policy Attacker: Crafting Adversarial Attacks for Diffusion-based Policies [[paper](https://arxiv.org/pdf/2405.19424)]
- [2024] Imagination Policy: Using Generative Point Cloud Models for Learning Manipulation Policies [[paper](https://arxiv.org/pdf/2406.11740)]
- [2024] Equivariant diffusion policy [[paper](https://arxiv.org/pdf/2407.01812)]
- [2024] Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation [[paper](https://arxiv.org/pdf/2409.14411)]
- [2024] Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation [[paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Hierarchical_Diffusion_Policy_for_Kinematics-Aware_Multi-Task_Robotic_Manipulation_CVPR_2024_paper.pdf)]
- [2024] Equivariant diffusion policy [[paper](https://arxiv.org/pdf/2407.01812)]
- [2024] Learning universal policies via text-guided video generation [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/1d5b9233ad716a43be5c0d3023cb82d0-Paper-Conference.pdf)]
- [2024] Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning [[paper](https://arxiv.org/pdf/2307.01849)]
- [2024] 3D Diffuser Actor: Policy Diffusion with 3D Scene Representations [[paper](https://arxiv.org/pdf/2402.10885)]
- [2024] Act3D: 3D Feature Field Transformers for Multi-Task Robotic Manipulation [[paper](https://arxiv.org/pdf/2306.17817)]
- [2024] GenDP: 3D Semantic Fields for Category-Level Generalizable Diffusion Policy [[paper](https://arxiv.org/pdf/2410.17488)]
- [2024] Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for Robust 3D Robotic Manipulation [[paper](https://arxiv.org/pdf/2411.18623)]
- [2024] Prediction with Action: Visual Policy Learning via Joint Denoising Process [[paper](https://arxiv.org/pdf/2411.18179)]
- [2024] Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations [[paper](https://arxiv.org/pdf/2412.14803)]



### 2023

- [2023] Diffusion policy: Visuomotor policy learning via action diffusion [[paper](https://arxiv.org/pdf/2303.04137)]